{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvharsh/Deep-Learning/blob/main/Bag%20of%20words/Bag_of_words_FILE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLdCrHshFAFy",
        "outputId": "a134e927-613f-4c82-d8ee-221e7a71836a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPAr5h6CFnuP",
        "outputId": "8a9724a5-3447-4187-97d6-4b45b3dc1b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Assuming you've already set the file_path variable to the path of your text file\n",
        "file_path = \"/content/drive/MyDrive/bagofwords.txt\"\n",
        "\n",
        "# Function to preprocess and print unique words from a text file\n",
        "def print_unique_words(file_path):\n",
        "    # Read the content of the file\n",
        "    with open(file_path, 'r') as file:\n",
        "        paragraph = file.read()\n",
        "\n",
        "    # Tokenize the paragraph\n",
        "    tokens = word_tokenize(paragraph)\n",
        "\n",
        "    # Convert to lower case to ensure case-insensitivity\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove brackets, commas, quotations, and punctuation marks\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove empty strings\n",
        "    tokens = list(filter(None, tokens))\n",
        "\n",
        "    # Remove duplicate words by converting the list to a set, then back to a list\n",
        "    unique_tokens = list(set(tokens))\n",
        "\n",
        "    # Print the unique tokens\n",
        "    print(unique_tokens)\n",
        "\n",
        "# Call the function with the path to your file\n",
        "print_unique_words(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VutABIafFGAB",
        "outputId": "2951413e-ca62-4164-f89c-04f97ccf2fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tortoise', 'made', 'wizard', 'school', 'beanstalk', 'summit', 'like', 'repairing', 'vanished', 'table', 'dozing', 'highest', 'blow', 'welcomed', 'past', 'goldilocks', 'thank', 'dream', 'fix', 'would', 'friend', 'truth', 'go', 'point', 'shot', 'cross', 'ca', 'way', 'united', 'hut', 'appeared', 'well', 'till', 'away', 'castle', 'bowl', 'leaped', 'day', 'one', 'eat', 'troll', 'could', 'every', 'matthew', 'opened', 'going', 'suspicion', 'golden', 'window', 'open', 'uses', 'young', 'kindle', 'finished', 'lily', 'looked', 'rush', 'line', 'magician', 'trace', 'ability', 'new', 'inside', 'given', 'whoosh', 'tell', 'began', 'friends', 'exactly', 'attempted', 'completed', 'since', 'also', 'upon', 'family', 'magic', 'returned', 'items', 'bump', 'wonderful', 'shared', 'three', 'chase', 'old', 'suddenly', 'door', 'house', 'light', 'ate', 'starting', 'path', 'playing', 'shine', 'front', 'wicked', 'music', 'must', 'possibly', 'girl', 'said', 'refuge', 'instantly', 'secret', 'good', 'race', 'bridge', 'hiding', 'things', 'lunch', 'took', 'sit', 'disappeared', 'goodbye', 'first', 'magical', 'vicious', 'visit', 'best', 'grumpy', 'lake', 'help', 'within', 'something', 'choose', 'decision', 'middle', 'moved', 'later', 'via', 'however', 'eggs', 'seat', 'landed', 'pigs', 'big', 'time', 'mysterious', 'climbing', 'small', 'met', 'see', 'billy', 'book', 'saw', 'actions', 'read', 'knocked', 'home', 'beautiful', 'community', 'story', 'whether', 'along', 'situated', 'chasing', 'failed', 'expands', 'result', 'named', 'exhausted', 'quickly', 'nt', 'goat', 'goose', 'hare', 'entered', 'satisfied', 'reading', 'grin', 'waved', 'offer', 'large', 'children', 'exclaimed', 'favourite', 'brightly', 'reached', 'without', 'slowly', 'wolf', 'far', 'summary', 'came', 'ran', 'near', 'laid', 'thought', 'harp', 'oatmeal', 'holding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Assuming you've already set the file_path variable to the path of your text file\n",
        "file_path = \"/content/drive/MyDrive/bagofwords.txt\"\n",
        "# Set the output file path where the unique words will be stored\n",
        "output_file_path = \"/content/uniquewords\"\n",
        "\n",
        "# Function to preprocess and store unique words from a text file to another file\n",
        "def store_unique_words(input_file_path, output_file_path):\n",
        "    # Read the content of the file\n",
        "    with open(input_file_path, 'r') as file:\n",
        "        paragraph = file.read()\n",
        "\n",
        "    # Tokenize the paragraph\n",
        "    tokens = word_tokenize(paragraph)\n",
        "\n",
        "    # Convert to lower case to ensure case-insensitivity\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove brackets, commas, quotations, and punctuation marks\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove empty strings\n",
        "    tokens = list(filter(None, tokens))\n",
        "\n",
        "    # Remove duplicate words by converting the list to a set, then back to a list\n",
        "    unique_tokens = list(set(tokens))\n",
        "\n",
        "    # Write the unique tokens to the output file\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        for token in unique_tokens:\n",
        "            file.write(token + '\\n')\n",
        "\n",
        "    print(f\"Unique words have been stored in {output_file_path}\")\n",
        "\n",
        "# Call the function with the path to your file\n",
        "store_unique_words(file_path, output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c93954-82c0-4ccb-f328-ff92992647f3",
        "id": "xLqzzCItK8XW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words have been stored in /content/uniquewords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}