{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvharsh/Deep-Learning/blob/main/Bag%20of%20words/Bag_of_words_COUNT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHFTvcYqKF3M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read the input text file\n",
        "def read_input_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "9nezxux5KrhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create bag of words representation\n",
        "def create_bag_of_words(text):\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "    # Fit tokenizer on the text\n",
        "    tokenizer.fit_on_texts([text])\n",
        "\n",
        "    # Convert text to sequence of word indices\n",
        "    sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "    # Create bag of words representation\n",
        "    bag_of_words = np.zeros((len(sequences), len(tokenizer.word_index) + 1))\n",
        "    for i, word_index in enumerate(sequences):\n",
        "        bag_of_words[i, word_index] = 1\n",
        "\n",
        "    # Get word counts\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    return bag_of_words, word_counts"
      ],
      "metadata": {
        "id": "goP5Ctf8Ktfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = read_input_file('input.txt')\n",
        "bag_of_words, word_counts = create_bag_of_words(input_text)\n",
        "\n",
        "# Print word counts\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word} : {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qv7ra31Kytc",
        "outputId": "187d94b6-9613-441f-a84b-162f24537f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep : 8\n",
            "learning : 18\n",
            "is : 7\n",
            "a : 10\n",
            "subset : 1\n",
            "of : 17\n",
            "machine : 6\n",
            "that : 3\n",
            "deals : 1\n",
            "with : 6\n",
            "algorithms : 4\n",
            "inspired : 1\n",
            "by : 2\n",
            "the : 8\n",
            "structure : 1\n",
            "and : 21\n",
            "function : 1\n",
            "brain's : 1\n",
            "neural : 6\n",
            "networks : 7\n",
            "these : 3\n",
            "learn : 2\n",
            "from : 2\n",
            "large : 1\n",
            "amounts : 1\n",
            "data : 8\n",
            "to : 10\n",
            "perform : 1\n",
            "tasks : 8\n",
            "such : 10\n",
            "as : 11\n",
            "image : 2\n",
            "speech : 1\n",
            "recognition : 3\n",
            "are : 12\n",
            "building : 1\n",
            "blocks : 1\n",
            "models : 1\n",
            "they : 3\n",
            "consist : 1\n",
            "interconnected : 1\n",
            "layers : 1\n",
            "nodes : 1\n",
            "neurons : 1\n",
            "process : 2\n",
            "input : 3\n",
            "convolutional : 1\n",
            "cnns : 1\n",
            "particularly : 1\n",
            "effective : 1\n",
            "for : 9\n",
            "due : 1\n",
            "their : 2\n",
            "ability : 1\n",
            "capture : 1\n",
            "spatial : 1\n",
            "hierarchies : 1\n",
            "recurrent : 1\n",
            "rnns : 1\n",
            "another : 1\n",
            "type : 3\n",
            "network : 1\n",
            "commonly : 1\n",
            "used : 6\n",
            "in : 8\n",
            "natural : 3\n",
            "language : 4\n",
            "processing : 2\n",
            "can : 1\n",
            "sequences : 1\n",
            "suitable : 1\n",
            "like : 1\n",
            "translation : 1\n",
            "text : 2\n",
            "generation : 1\n",
            "generative : 1\n",
            "adversarial : 1\n",
            "gans : 2\n",
            "model : 2\n",
            "consists : 1\n",
            "two : 1\n",
            "generator : 1\n",
            "discriminator : 1\n",
            "which : 1\n",
            "trained : 3\n",
            "simultaneously : 1\n",
            "generating : 1\n",
            "realistic : 1\n",
            "images : 1\n",
            "augmentation : 1\n",
            "transfer : 1\n",
            "technique : 1\n",
            "where : 2\n",
            "pre : 1\n",
            "starting : 1\n",
            "point : 1\n",
            "new : 1\n",
            "task : 1\n",
            "it : 4\n",
            "allows : 1\n",
            "faster : 1\n",
            "training : 2\n",
            "better : 1\n",
            "performance : 2\n",
            "especially : 1\n",
            "when : 1\n",
            "dealing : 1\n",
            "limited : 1\n",
            "labeled : 1\n",
            "autoencoders : 1\n",
            "reconstruct : 1\n",
            "compression : 1\n",
            "feature : 1\n",
            "support : 1\n",
            "vector : 1\n",
            "machines : 1\n",
            "svms : 1\n",
            "decision : 1\n",
            "trees : 1\n",
            "random : 1\n",
            "forests : 1\n",
            "widely : 1\n",
            "classification : 2\n",
            "regression : 1\n",
            "often : 1\n",
            "employed : 1\n",
            "conjunction : 1\n",
            "techniques : 2\n",
            "improved : 1\n",
            "nlp : 1\n",
            "field : 2\n",
            "study : 1\n",
            "focused : 1\n",
            "on : 1\n",
            "interaction : 1\n",
            "between : 1\n",
            "computers : 1\n",
            "human : 1\n",
            "languages : 1\n",
            "involves : 1\n",
            "sentiment : 1\n",
            "analysis : 1\n",
            "named : 1\n",
            "entity : 1\n",
            "reinforcement : 3\n",
            "an : 2\n",
            "agent : 1\n",
            "learns : 1\n",
            "make : 1\n",
            "decisions : 1\n",
            "interacting : 1\n",
            "environment : 1\n",
            "applications : 2\n",
            "game : 1\n",
            "playing : 2\n",
            "robotics : 1\n",
            "autonomous : 1\n",
            "driving : 2\n",
            "combines : 1\n",
            "complex : 1\n",
            "behaviors : 1\n",
            "directly : 1\n",
            "raw : 1\n",
            "sensory : 1\n",
            "has : 1\n",
            "achieved : 1\n",
            "remarkable : 1\n",
            "success : 1\n",
            "domains : 1\n",
            "video : 1\n",
            "games : 1\n",
            "controlling : 1\n",
            "robotic : 1\n",
            "systems : 3\n",
            "artificial : 1\n",
            "intelligence : 1\n",
            "ai : 4\n",
            "encompasses : 1\n",
            "other : 1\n",
            "approaches : 1\n",
            "creating : 1\n",
            "intelligent : 1\n",
            "technologies : 2\n",
            "have : 1\n",
            "diverse : 1\n",
            "areas : 2\n",
            "healthcare : 1\n",
            "finance : 1\n",
            "transportation : 1\n",
            "ethical : 1\n",
            "considerations : 1\n",
            "crucial : 1\n",
            "development : 1\n",
            "deployment : 1\n",
            "issues : 1\n",
            "bias : 1\n",
            "privacy : 1\n",
            "concerns : 1\n",
            "algorithmic : 1\n",
            "transparency : 1\n",
            "need : 1\n",
            "be : 1\n",
            "addressed : 1\n",
            "ensure : 1\n",
            "responsible : 1\n",
            "use : 1\n",
            "future : 1\n",
            "holds : 1\n",
            "promise : 1\n",
            "advancements : 1\n",
            "self : 1\n",
            "cars : 1\n",
            "personalized : 1\n",
            "medicine : 1\n",
            "understanding : 1\n",
            "research : 1\n",
            "efforts : 1\n",
            "continue : 1\n",
            "push : 1\n",
            "boundaries : 1\n",
            "what : 1\n",
            "possible : 1\n"
          ]
        }
      ]
    }
  ]
}